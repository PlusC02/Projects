# -*- coding: utf-8 -*-
"""Block.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V8ZZYzh03A0h-UP-4xiAODNyfljoJdvi
"""

import torch
import torch.nn as nn
from torch.nn import functional as F

from LayerNorm import LayerNorm
from MaskedMultiheadAttention import Masked_MultiHeadAttention
from NeuralNetwork import Position_wise_FFN

class decoder_block(nn.Module):
  """
  We define a decoder block to improve the readability for the last model
  decoder_block would be placed 6 times.
  Attention -> FFN with Layer norm

  """
  def __init__(self,embed_dim,num_heads,dropout):
    super().__init__()
    self.multiheadattn = Masked_MultiHeadAttention(embed_dim,num_heads,dropout)
    self.FFN = Position_wise_FFN(embed_dim,dropout)
    self.layernorm1 = LayerNorm(embed_dim)
    self.layernorm2 = LayerNorm(embed_dim)

  def forward(self,x): # Residual connection
    x = self.layernorm1(self.multiheadattn(x,x,x)+x) # LayerNorm(x + Sublayer(x))
    x = self.layernorm2(self.FFN(x)+x)
    return x