# -*- coding: utf-8 -*-
"""MultiHeadAttention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17nKRvc7dfkMCkiSrcxU6ghZtWm8ZZGJe
"""

import torch
import torch.nn as nn
from torch.nn import functional as F

class MultiHeadAttention(nn.Module):
  """
  The self attention mechanism that can be single head / multi-head
  Input should be (x,x,x) for self attention
  Input should be (x,x,y) for cross attention

  Thinking process:
  Suppose the input is (B,S,D)
  Let input be x (B,S,D)
  H = head in our case
  key = x @ key Matrix => (B,S,D) @ (D,H) => (B,S,H)*D/H
  query = x @ query Matrix => (B,S,D) @ (D,H) => (B,S,H)
  Qeury @ key transpose = (B,S,H) @ (B,H,S) = (B,S,S) Cannot reverse order
  value = x @ value Matrix = (B,S,H)
  score @ value = (B,S,H)
  """
  def __init__(self,embed_dim =512, heads = 8, dropout = 0.2): # Following the same as the paper
    super(MultiHeadAttention,self).__init__()
    self.embed_dim = embed_dim
    self.heads = heads
    self.head = embed_dim//heads  # head_dim
    assert self.head*self.heads == embed_dim


    # To maintain a correct size of matrix
    self.key = nn.Linear(embed_dim, embed_dim, bias=False)
    self.query = nn.Linear(embed_dim, embed_dim, bias=False)
    self.value = nn.Linear(embed_dim, embed_dim, bias=False)

    # Output Projection
    self.fc_out = nn.Linear(embed_dim, embed_dim)
    self.dropout = nn.Dropout(dropout)

  def forward(self,key,query,value,mask = None):
    #
    B,S,D = query.shape
    # self.key(x) -> (B,S,D) so we need to make it (B,S,Num_heads,head_dim)
    key = self.key(key).view(B,S,self.heads,self.head).transpose(1,2) # (B,S,D)->(B,Num_heads,S,Head_dim)
    query = self.query(query).view(B,S,self.heads,self.head).transpose(1,2)
    value = self.value(value).view(B,S,self.heads,self.head).transpose(1,2)

    #Attention score
    score = torch.matmul(query, key.transpose(-1,-2)) / math.sqrt(self.head) # (B,Num_heads,S,S)

    if mask is not None:
      tril = torch.tril(torch.ones(S, S)).to(device) # Usually we should register buffer, but need to create one more variable for initialization
      score = score.masked_fill(tril==0, float('-inf')) #(S,S)

    score = F.softmax(score, dim=-1) #(S,S) broadcast to (B,Num_heads,S,S)


    weight = self.dropout(score)

    result = torch.matmul(weight, value) #weight @ value #(B,Num_heads,S,S) @(B,Num_heads,S,Head_dim)-> (B,Num_heads,S,Head_dim)
    result = result.transpose(1,2).contiguous().view(B,S,D) # result = result.transpose(1, 2).contiguous().view(B, S, self.embed_dim)

    return self.fc_out(result)